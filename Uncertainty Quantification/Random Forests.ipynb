{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of UQ - Accuracy Rejection Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from math import log\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty from Model 1, Model 1+2, Model 1+2+3, \n",
    "# instead of Model 1, Model 2, Model 3, \n",
    "# because n+1 Model always belong to Model n\n",
    "# in an additive manner\n",
    "\n",
    "# catboost delivers already laplace_smoothed_proba_predictions\n",
    "\n",
    "\n",
    "def uncertainty(estimators, test_data):\n",
    "    # function expects a list of estimators and test data \n",
    "    # and returns aleatoric, epistemic and total uncertainty\n",
    "    # for each prediction\n",
    "    \n",
    "\n",
    "    labels = [0,1] # labels are 0 or 1 for this task \n",
    "    num_trees = len(estimators) # number of trees\n",
    "    num_samples =  len(test_data) # number of samples\n",
    "    predictions = [] # init predictions\n",
    "    \n",
    "    \n",
    "    # calculate laplace corrected predictions\n",
    "    # for each estimator and test sample\n",
    "    \n",
    "    for m in estimators:\n",
    "        leave_id = m.apply(x_test)\n",
    "        laplace_corrected_prediction = []\n",
    "        \n",
    "        for frequencies in [el[0] for el in m.tree_.value[leave_id]]:\n",
    "            first_class_proba = (frequencies[0]+1)/(frequencies[0]+frequencies[1]+2)\n",
    "            second_class_proba = (frequencies[1]+1)/(frequencies[0]+frequencies[1]+2)                  \n",
    "            laplace_corrected_prediction.append([first_class_proba,second_class_proba])\n",
    "            \n",
    "        predictions.append(laplace_corrected_prediction)\n",
    "        \n",
    "    # init uncertaintys\n",
    "    total = []\n",
    "    aleatoric = []\n",
    "    epistemic = []\n",
    "    \n",
    "    # ALEATORIC UNCERTAINTY\n",
    "    for i in range(num_samples):\n",
    "        result = 0\n",
    "        for m in range(num_trees):\n",
    "            for y in labels:\n",
    "                p = predictions[m][i][y]\n",
    "                result = result + (p * log(p,2))\n",
    "        \n",
    "        final = -(1/num_trees) * result\n",
    "\n",
    "        aleatoric.append(final) \n",
    "\n",
    "    p_sum = 0\n",
    "    for i in range(num_samples):\n",
    "        result = 0\n",
    "        for y in labels:\n",
    "            p_sum = 0\n",
    "            for m in range(num_trees):\n",
    "                \n",
    "                p = predictions[m][i][y]\n",
    "                p_sum = p + p_sum\n",
    "            \n",
    "            p_avg_sum = ((1/num_trees)* p_sum)\n",
    "            p_logged_avg_sum = p_avg_sum*log(p_avg_sum,2)\n",
    "            result = result + p_logged_avg_sum\n",
    "            \n",
    "        result = result *(-1)\n",
    "        total.append(result)\n",
    "        epistemic.append(result - aleatoric[i])\n",
    "    \n",
    "    return aleatoric, epistemic, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_matrix(model, x_test, n_estimators, laplace_smoothing, log=False):\n",
    "    porb_matrix = [[[] for j in range(n_estimators)] for i in range(x_test.shape[0])]\n",
    "    for etree in range(n_estimators):\n",
    "        # populate the porb_matrix with the tree_prob\n",
    "        tree_prob = model.estimators_[etree].predict_proba(x_test)\n",
    "        if laplace_smoothing > 0:\n",
    "            leaf_index_array = model.estimators_[etree].apply(x_test)\n",
    "            for data_index, leaf_index in enumerate(leaf_index_array):\n",
    "                leaf_values = model.estimators_[etree].tree_.value[leaf_index]\n",
    "                leaf_samples = np.array(leaf_values).sum()\n",
    "                for i,v in enumerate(leaf_values[0]):\n",
    "                    tmp = (v + laplace_smoothing) / (leaf_samples + (len(leaf_values[0]) * laplace_smoothing))\n",
    "                    # print(f\">>>>>>>>>>>>>>> {tmp}  data_index {data_index} prob_index {i} v {v} leaf_samples {leaf_samples}\")\n",
    "                    tree_prob[data_index][i] = (v + laplace_smoothing) / (leaf_samples + (len(leaf_values[0]) * laplace_smoothing))\n",
    "                # exit()\n",
    "\n",
    "        for data_index, data_prob in enumerate(tree_prob):\n",
    "            porb_matrix[data_index][etree] = list(data_prob)\n",
    "\n",
    "        if log:\n",
    "            print(f\"----------------------------------------[{etree}]\")\n",
    "            print(f\"class {model.estimators_[etree].predict(x_test)}  prob \\n{tree_prob}\")\n",
    "    return porb_matrix\n",
    "\n",
    "def uncertainty_estimate(probs): # three dimentianl array with d1 as datapoints, (d2) the rows as samples and (d3) the columns as probability for each class\n",
    "    p = np.array(probs)\n",
    "    entropy = -p*np.ma.log10(p)\n",
    "    entropy = entropy.filled(0)\n",
    "    a = np.sum(entropy, axis=1)\n",
    "    a = np.sum(a, axis=1) / entropy.shape[1]\n",
    "    p_m = np.mean(p, axis=1)\n",
    "    total = -np.sum(p_m*np.ma.log10(p_m), axis=1)\n",
    "    total = total.filled(0)\n",
    "    e = total - a\n",
    "    return total, e, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('SPECT .train', delimiter=',', header=None)\n",
    "test = pd.read_csv('SPECT .test', delimiter=',',header=None)\n",
    "data = pd.concat([train,test]).reset_index(drop=True)\n",
    "x = data.drop(0, axis=1)\n",
    "y = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7901234567901234\n",
      "Accuracy = 0.7777777777777778\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.8518518518518519\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.7530864197530864\n",
      "Accuracy = 0.7654320987654321\n",
      "Accuracy = 0.9012345679012346\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.8765432098765432\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.7654320987654321\n",
      "Accuracy = 0.8518518518518519\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.7777777777777778\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.7901234567901234\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.8641975308641975\n",
      "Accuracy = 0.8518518518518519\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.8641975308641975\n",
      "Accuracy = 0.7654320987654321\n",
      "Accuracy = 0.7901234567901234\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.7777777777777778\n",
      "Accuracy = 0.7901234567901234\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.7901234567901234\n",
      "Accuracy = 0.8518518518518519\n",
      "Accuracy = 0.7654320987654321\n",
      "Accuracy = 0.7901234567901234\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.8518518518518519\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.7530864197530864\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.7901234567901234\n",
      "Accuracy = 0.8518518518518519\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.8148148148148148\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.8148148148148148\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.7901234567901234\n",
      "Accuracy = 0.8765432098765432\n",
      "Accuracy = 0.7160493827160493\n",
      "Accuracy = 0.7901234567901234\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.8148148148148148\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.8148148148148148\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.9012345679012346\n",
      "Accuracy = 0.7901234567901234\n",
      "Accuracy = 0.8518518518518519\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.8148148148148148\n",
      "Accuracy = 0.8148148148148148\n",
      "Accuracy = 0.9135802469135802\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.8518518518518519\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.7901234567901234\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.8148148148148148\n",
      "Accuracy = 0.8765432098765432\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.7530864197530864\n",
      "Accuracy = 0.8765432098765432\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.8148148148148148\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.8395061728395061\n",
      "Accuracy = 0.8148148148148148\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.8271604938271605\n",
      "Accuracy = 0.7901234567901234\n",
      "Accuracy = 0.7901234567901234\n",
      "Accuracy = 0.8024691358024691\n",
      "Accuracy = 0.8641975308641975\n"
     ]
    }
   ],
   "source": [
    "num_runs = 100\n",
    "n_estimators = 50 # number of trees in the forest\n",
    "test_size = 0.3\n",
    "laplace_smoothing = 1\n",
    "unc_method = \"entropy\"\n",
    "\n",
    "\n",
    "ARC_eU_list, ARC_aU_list, ARC_random_list = [],[],[]\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    test_size=0.3\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size,shuffle=True)\n",
    "    x_train, x_test, y_train, y_test = x_train.copy(), x_test.copy(), y_train.copy(), y_test.copy()\n",
    "\n",
    "    model = RandomForestClassifier(bootstrap=True,\n",
    "                                        criterion='entropy',\n",
    "                                        max_depth=10,\n",
    "                                        n_estimators=n_estimators,\n",
    "                                        random_state=None,\n",
    "                                        verbose=0,\n",
    "                                        warm_start=False)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    preds_class = model.predict(x_test)\n",
    "    print(\"Accuracy = {}\".format(accuracy_score(y_test, preds_class)))\n",
    "\n",
    "    porb_matrix = get_prob_matrix(model, x_test, n_estimators, laplace_smoothing, log=0)\n",
    "    total_uncertainty, epistemic_uncertainty, aleatoric_uncertainty = uncertainty_estimate(np.array(porb_matrix))\n",
    "    \n",
    "    x_test['a']=aleatoric_uncertainty\n",
    "    x_test['e']=epistemic_uncertainty\n",
    "\n",
    "    num_rows = x_test.shape[0]\n",
    "    discard_count = 0\n",
    "    eU = x_test.sort_values('e', ascending=True)\n",
    "    aU = x_test.sort_values('a', ascending=True)\n",
    "\n",
    "    ARC_eU =  [preds_class]\n",
    "    ARC_aU = [preds_class]\n",
    "    ARC_random = [preds_class]\n",
    "\n",
    "    i = 0.05\n",
    "    while i < 1:\n",
    "        discard_count = int (num_rows*i)\n",
    "        eU_samples = eU.iloc[:-discard_count,0:22]\n",
    "        aU_samples = aU.iloc[:-discard_count,0:22]\n",
    "        random_samples = x_test.iloc[:,0:22].sample((num_rows-discard_count))\n",
    "        preds_class_eU = model.predict(eU_samples)\n",
    "        preds_class_aU = model.predict(aU_samples)\n",
    "        preds_class_random = model.predict(random_samples)\n",
    "        accuracy_eu = accuracy_score(y_test[eU_samples.index], preds_class_eU)\n",
    "        accuracy_au = accuracy_score(y_test[aU_samples.index], preds_class_aU)\n",
    "        accuracy_random = accuracy_score(y_test[random_samples.index], preds_class_random)\n",
    "        ARC_eU.append(accuracy_eu)\n",
    "        ARC_aU.append(accuracy_au)\n",
    "        ARC_random.append(accuracy_random)\n",
    "        i+=0.05\n",
    "    \n",
    "    ARC_eU_list.append(ARC_eU)\n",
    "    ARC_aU_list.append(ARC_aU)\n",
    "    ARC_random_list.append(ARC_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARC_eU_mean, ARC_aU_mean, ARC_random_mean = [],[],[]\n",
    "for i in range(len(ARC_eU_list[0])):\n",
    "    ARC_eU_mean.append(np.array([el[i] for el in ARC_eU_list]).mean())\n",
    "    ARC_aU_mean.append(np.array([el[i] for el in ARC_aU_list]).mean())\n",
    "    ARC_random_mean.append(np.array([el[i] for el in ARC_random_list]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reject   E    A    R \n",
      "0.05:  0.83 0.83 0.83 \n",
      "0.10:  0.82 0.83 0.82 \n",
      "0.15:  0.83 0.84 0.82 \n",
      "0.20:  0.83 0.85 0.82 \n",
      "0.25:  0.83 0.88 0.82 \n",
      "0.30:  0.84 0.90 0.82 \n",
      "0.35:  0.84 0.92 0.82 \n",
      "0.40:  0.85 0.93 0.82 \n",
      "0.45:  0.86 0.93 0.82 \n",
      "0.50:  0.86 0.94 0.82 \n",
      "0.55:  0.87 0.94 0.82 \n",
      "0.60:  0.88 0.94 0.82 \n",
      "0.65:  0.88 0.95 0.82 \n",
      "0.70:  0.88 0.95 0.82 \n",
      "0.75:  0.89 0.97 0.82 \n",
      "0.80:  0.89 0.97 0.82 \n",
      "0.85:  0.89 0.98 0.80 \n",
      "0.90:  0.91 0.99 0.82 \n",
      "0.95:  0.93 1.00 0.82 \n",
      "1.00:  0.98 1.00 0.84 \n"
     ]
    }
   ],
   "source": [
    "print('Reject   E    A    R ')\n",
    "i = 0.05\n",
    "for e,a,r in zip(ARC_eU_mean,ARC_aU_mean,ARC_random_mean):\n",
    "    print(f'{i:.2f}:  {e:.2f} {a:.2f} {r:.2f} ')\n",
    "    i +=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
